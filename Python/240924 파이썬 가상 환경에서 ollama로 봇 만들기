vsì½”ë“œ í„°ë¯¸ë„ì—ì„œ

python -m venv venv
.\venv\Scripts\activate

ExecutionPolicy ì˜¤ë¥˜ê°€ ëœ° ê²½ìš°
powershell ê´€ë¦¬ìë¡œ ì‹¤í–‰

Set-ExecutionPolicy unrestricted
A

vsì½”ë“œë¡œ ì™€ì„œ
pip install ollama

#local ollama inference
 
import ollama
 
response = ollama.chat(model='qwen2.5:7b', messages=[
    {'role': 'system', 'content': 'ë‹¹ì‹ ì€ ì¸ê³µì§€ëŠ¥ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì„œ ì¶œë ¥í•©ë‹ˆë‹¤.'},
    {'role': 'user', 'content': 'ë„ˆ ì´ë¦„ì´ ë­ë‹ˆ?'}
])
 
print(response['message']['content'])

------------------------------------------------------------------------------------------------------------------------
#ë²ˆì—­ë´‡.ipynb
import ollama
 
def generate_response(system_message, user_message, model="gemma2:9b", temperature=0, top_p=1, top_k=1):
    client = ollama.Client()
    response = client.chat(
        model=model,
        stream=True,
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ],
        options={
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
        }
    )
    full_response = ""    
    for chunk in response:
        content = chunk['message']['content']
        print(content, end='', flush=True)  # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
        full_response += content  # ì „ì²´ ì‘ë‹µ êµ¬ì„±      
    return full_response
 
# ì˜ˆì‹œ ì‚¬ìš©ë²•
system_msg = """
### Instruction
ë„ˆëŠ” ì§€ê¸ˆë¶€í„° ì „ë¬¸ ë²ˆì—­ê°€ì•¼  
ë„ˆì˜ ì—­í• ì€ ì£¼ì–´ì§„ ì–¸ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ê²ƒì´ì•¼.
ë„ˆëŠ” ì‚¬ìš©ìì˜ ì…ë ¥ì„ ì¶œë ¥í•˜ê³ , ê·¸ ì•„ë˜ì— ë²ˆì—­í•œ ë‚´ìš©ì„ ë„£ì–´ì¤˜.
 
ì›ë¬¸ : ì‚¬ìš©ì ì…ë ¥
ë²ˆì—­ : ì‚¬ìš©ì ì…ë ¥ì„ ë²ˆì—­í•œ ë‚´ìš©
 
ì¶œë ¥ í…œí”Œë¦¿ì€ ë‹¤ìŒê³¼ ê°™ì•„. ë°˜ë“œì‹œ ë²ˆì—­ ê²°ê³¼ë§Œ ì¶œë ¥í•´ì•¼í•´.
<ë²ˆì—­> My name is Hong-gil-dong.
"""
 
text = "ë‚˜ëŠ” ë°¥ì„ í•™ìƒì‹ë‹¹ì—ì„œ ë¨¹ì–´ì„œ ë°°ê°€ ë„ˆë¬´ ë¶ˆëŸ¬."
user_msg = f"""
### Question
 <ì›ë¬¸> {text}
 <ë²ˆì—­>
"""
 
response = generate_response(system_msg, user_msg, model="gemma2:9b", temperature=0.5, top_p=1, top_k=1)
# print(response)

------------------------------------------------------------------------------------------------------------------------
#ìš”ì•½ë´‡.ipynb
import ollama
 
def generate_response(system_message, user_message, model="gemma2:9b", temperature=0, top_p=1, top_k=1):
    client = ollama.Client()
    response = client.chat(
        model=model,
        stream=True,
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ],
        options={
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
        }
    )
    full_response = ""    
    for chunk in response:
        content = chunk['message']['content']
        print(content, end='', flush=True)  # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
        full_response += content  # ì „ì²´ ì‘ë‹µ êµ¬ì„±      
    return full_response
 
# ì˜ˆì‹œ ì‚¬ìš©ë²•
system_msg = """
### Instruction
ë„ˆëŠ” ì§€ê¸ˆë¶€í„° ì „ë¬¸ ìš”ì•½ê°€ì•¼.
ë„ˆì˜ ì—­í• ì€ ì£¼ì–´ì§„ ë¬¸ì¥ì„ ê°„ëµíˆ ìš”ì•½í•˜ëŠ” ê²ƒì´ì•¼.
ë„ˆëŠ” ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë¬¸ë‹¨ ë²ˆí˜¸ë¡œ ë‚˜ëˆ„ê³  ë¬¸ë‹¨ë§ˆë‹¤ ì£¼ì¥, ê·¼ê±°, ìš”ì•½ì„ ì •ë¦¬í•˜ëŠ” ê±°ì•¼.
JSONì˜ ê¸°ë³¸ êµ¬ì¡°ë¡œ ë‹µë³€í•˜ê³  ë¬¸ë‹¨ë²ˆí˜¸, ì£¼ì¥, ê·¼ê±°, ìš”ì•½ì´ ë“¤ì–´ê°„ë‹¤.
"""
 
text = """ì¸ê³µì§€ëŠ¥ì˜ ë°œì „ê³¼ ì§ì—… ë° ì†Œë“ì˜ ë³€í™”
 
ì¸ê³µì§€ëŠ¥(AI)ì˜ ë°œì „ì€ ì§ì—…ê³¼ ì†Œë“ êµ¬ì¡°ì— í° ë³€í™”ë¥¼ ì¼ìœ¼í‚¤ê³  ìˆìŠµë‹ˆë‹¤. ë…¸ë™ ì‹œì¥ì˜ ì¬í¸ì€ ì§ì—…ì˜ ì„±ê²©ê³¼ ì†Œë“ ë¶„í¬ì— ì‹¬ëŒ€í•œ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ì—ì„¸ì´ì—ì„œëŠ” AIê°€ ì§ì—…ê³¼ ì†Œë“ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ê³ , ì´ì— ëŒ€í•œ ëŒ€ì‘ ë°©ì•ˆì„ ì œì‹œí•˜ê³ ì í•©ë‹ˆë‹¤.
 
AI ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ë‹¨ìˆœí•˜ê³  ë°˜ë³µì ì¸ ì‘ì—…ì€ AIì™€ ë¡œë´‡ì— ì˜í•´ ëŒ€ì²´ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì œì¡°ì—…, ë¬¼ë¥˜, ì„œë¹„ìŠ¤ ì—…ì¢…ì—ì„œ ë¡œë´‡ì´ ì¸ê°„ì˜ ë…¸ë™ì„ ëŒ€ì‹ í•˜ë©°, ì´ëŠ” í•´ë‹¹ ë¶„ì•¼ì˜ ì¼ìë¦¬ ê°ì†Œë¡œ ì´ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤.
 
ê·¸ëŸ¬ë‚˜ AIëŠ” ìƒˆë¡œìš´ ì§ì—… ê¸°íšŒë„ ì°½ì¶œí•©ë‹ˆë‹¤. ë°ì´í„° ê³¼í•™ì, ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ ë“±ì˜ ì§ì—…ì´ ìƒê²¨ë‚¬ìœ¼ë©°, ììœ¨ì£¼í–‰ì°¨, ìŠ¤ë§ˆíŠ¸ í—¬ìŠ¤ì¼€ì–´ ë“± AI ê¸°ìˆ ì„ í™œìš©í•œ ìƒˆë¡œìš´ ì‚°ì—…ì—ì„œë„ ë§ì€ ì¼ìë¦¬ê°€ ì°½ì¶œë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€í™”ëŠ” ì§ì—…ì˜ ì¬êµìœ¡ê³¼ ì „í™˜ì„ í•„ìš”ë¡œ í•˜ë©°, ìƒˆë¡œìš´ ê¸°ìˆ  ìŠµë“ì˜ ì¤‘ìš”ì„±ì´ ì»¤ì§€ê³  ìˆìŠµë‹ˆë‹¤.
 
AI ë„ì…ì€ ì†Œë“ ë¶„í¬ì—ë„ í° ë³€í™”ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. ê³ ë„ë¡œ ìˆ™ë ¨ëœ ê¸°ìˆ ì§ì˜ ìˆ˜ìš” ì¦ê°€ë¡œ ì´ë“¤ ì§ì—…ì˜ ì†Œë“ì€ ìƒìŠ¹í•˜ê³  ìˆì§€ë§Œ, ìë™í™”ë¡œ ëŒ€ì²´ ê°€ëŠ¥í•œ ì§ì—…ì— ì¢…ì‚¬í•˜ë˜ ë…¸ë™ìë“¤ì€ ì‹¤ì§í•˜ê±°ë‚˜ ë‚®ì€ ì„ê¸ˆì„ ë°›ê²Œ ë˜ì–´ ì†Œë“ ë¶ˆê· í˜•ì´ ì‹¬í™”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë˜í•œ AI ê¸°ì—…ì˜ ë¶€ìƒìœ¼ë¡œ ì¼ë¶€ ê¸°ì—…ê°€ì™€ íˆ¬ììë“¤ì€ ë§‰ëŒ€í•œ ìˆ˜ìµì„ ì˜¬ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ìƒìœ„ 1%ì˜ ì†Œë“ì´ ì¦ê°€í•˜ê³ , ì¤‘ì‚°ì¸µê³¼ í•˜ìœ„ ê³„ì¸µì˜ ì†Œë“ì€ ìƒëŒ€ì ìœ¼ë¡œ ì •ì²´ë˜ê±°ë‚˜ ê°ì†Œí•˜ëŠ” ì–‘ìƒì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì†Œë“ ë¶ˆê· í˜•ì€ ì‚¬íšŒì  ë¶ˆì•ˆì •ì„±ì„ ì´ˆë˜í•  ìˆ˜ ìˆìœ¼ë©°, ì´ì— ëŒ€í•œ ëŒ€ì±… ë§ˆë ¨ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
 
AIì˜ ë°œì „ì— ë”°ë¥¸ ì§ì—…ê³¼ ì†Œë“ì˜ ë³€í™”ì— ëŒ€ì‘í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ì¤‘ìš”í•œ ë°©ì•ˆì´ í•„ìš”í•©ë‹ˆë‹¤. ì²«ì§¸, êµìœ¡ê³¼ ì¬êµìœ¡ í”„ë¡œê·¸ë¨ì„ ê°•í™”í•´ì•¼ í•©ë‹ˆë‹¤. ë¹ ë¥´ê²Œ ë³€í™”í•˜ëŠ” ê¸°ìˆ  í™˜ê²½ì— ì ì‘í•˜ê¸° ìœ„í•´ì„œëŠ” ì§€ì†ì ì¸ í•™ìŠµì´ í•„ìš”í•˜ë©°, ì •ë¶€ì™€ ê¸°ì—…ì€ ì´ë¥¼ ì§€ì›í•˜ëŠ” ì²´ê³„ë¥¼ êµ¬ì¶•í•´ì•¼ í•©ë‹ˆë‹¤.
ë‘˜ì§¸, ì‚¬íšŒ ì•ˆì „ë§ì„ ê°•í™”í•´ì•¼ í•©ë‹ˆë‹¤. ì¼ìë¦¬ ë³€ë™ì„±ê³¼ ë¶ˆí™•ì‹¤ì„±ì´ ì¦ê°€í•˜ëŠ” ì‹œëŒ€ì— ì‹¤ì§ìì™€ ì €ì†Œë“ì¸µì„ ë³´í˜¸í•˜ê¸° ìœ„í•œ ì‚¬íšŒ ì•ˆì „ë§ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ê¸°ë³¸ì†Œë“ì œ ë„ì…, ì‹¤ì—…ë³´í—˜ ê°•í™”, ì§ì—… ì¬êµìœ¡ ì§€ì› ë“±ì˜ ë°©ì•ˆì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
 
ì…‹ì§¸, ì†Œë“ ë¶ˆê· í˜•ì„ í•´ì†Œí•˜ê¸° ìœ„í•œ ì •ì±…ì´ í•„ìš”í•©ë‹ˆë‹¤. ì„¸ê¸ˆ ì •ì±…ì„ í†µí•´ ê³ ì†Œë“ì¸µê³¼ ê¸°ì—…ì˜ ì´ìµì„ ê³µì •í•˜ê²Œ ë¶„ë°°í•˜ê³ , ì¤‘ì‚°ì¸µê³¼ ì €ì†Œë“ì¸µì˜ ì†Œë“ì„ ì¦ëŒ€ì‹œí‚¤ëŠ” ë°©ì•ˆì„ ë§ˆë ¨í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ ê¸°ì—…ì€ ì‚¬íšŒì  ì±…ì„ì„ ë‹¤í•˜ê³  ê³µì •í•œ ë…¸ë™ í™˜ê²½ì„ ì¡°ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
 
ì¸ê³µì§€ëŠ¥ì˜ ë°œì „ì€ ì§ì—…ê³¼ ì†Œë“ êµ¬ì¡°ì— í° ë³€í™”ë¥¼ ì´ˆë˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìë™í™”ì™€ ìƒˆë¡œìš´ ê¸°ìˆ ì˜ ë„ì…ìœ¼ë¡œ ì¼ë¶€ ì§ì—…ì€ ì‚¬ë¼ì§€ê³ , ìƒˆë¡œìš´ ì§ì—…ì´ ìƒê²¨ë‚˜ë©°, ì†Œë“ ë¶ˆê· í˜•ì´ ì‹¬í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€í™”ì— íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ì‘í•˜ê¸° ìœ„í•´ì„œëŠ” êµìœ¡ê³¼ ì¬êµìœ¡, ì‚¬íšŒ ì•ˆì „ë§ ê°•í™”, ì†Œë“ ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•œ ì •ì±…ì´ í•„ìš”í•©ë‹ˆë‹¤. AI ì‹œëŒ€ì˜ ë„ë˜ëŠ” ë„ì „ê³¼ ê¸°íšŒë¥¼ ë™ì‹œì— ì œê³µí•˜ë©°, ì´ë¥¼ ì˜ í™œìš©í•˜ëŠ” ê²ƒì´ ì•ìœ¼ë¡œì˜ ì‚¬íšŒì  ì•ˆì •ê³¼ ë²ˆì˜ì„ ìœ„í•œ ì—´ì‡ ê°€ ë  ê²ƒì…ë‹ˆë‹¤."""
user_msg = f"""
### Question
 <ì›ë¬¸> {text}
"""
 
response = generate_response(system_msg, user_msg, model="gemma2:9b", temperature=0.5, top_p=1, top_k=1)
# print(response)
------------------------------------------------------------------------------------------------------------------------
#ê°ì •ë¶„ë¥˜.ipynb
import ollama
 
def generate_response(system_message, user_message, model="gemma2:9b", temperature=0, top_p=1, top_k=1):
    client = ollama.Client()
    response = client.chat(
        model=model,
        stream=True,
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ],
        options={
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
        }
    )
    full_response = ""    
    for chunk in response:
        content = chunk['message']['content']
        print(content, end='', flush=True)  # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
        full_response += content  # ì „ì²´ ì‘ë‹µ êµ¬ì„±      
    return full_response
 
# ì˜ˆì‹œ ì‚¬ìš©ë²•
system_msg = """
### Instruction
ë„ˆëŠ” ì§€ê¸ˆë¶€í„° ê°ì • ë¶„ë¥˜ ì „ë¬¸ê°€ì•¼.
ë„ˆì˜ ì—­í• ì€ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì¥ì„ íŒŒì•…í•´ì„œ ì–´ë–¤ ê°ì •ì¸ì§€ íŒë‹¨í•˜ë©´ ë¼.
JSONì˜ ê¸°ë³¸ êµ¬ì¡°ë¡œ ë‹µë³€í•˜ê³  ë¬¸ì¥, ê°ì •ì´ ë“¤ì–´ê°„ë‹¤.
ê°ì •ì˜ ì¢…ë¥˜ëŠ” ê¸ì •, ë¶€ì •, ì¤‘ë¦½ì´ ìˆë‹¤.
"""
 
text = """ë‚˜ëŠ” ì´ ìŒì‹ì´ ê·¸ëƒ¥ ê·¸ë ‡ë‹¤ê³  ìƒê°í•´.

ë‚˜ëŠ” ì´ ì˜í™”ê°€ ì •ë§ ì¬ë¯¸ì—†ì–´.

ë‚˜ëŠ” ìš°ë¦¬ ëŒ€í•™êµ í•™ì‹ì´ ë§›ìˆì–´.
"""
user_msg = f"""
### Question
 <ì›ë¬¸> {text}
"""
 
response = generate_response(system_msg, user_msg, model="gemma2:9b", temperature=0.5, top_p=1, top_k=1)
# print(response)
------------------------------------------------------------------------------------------------------------------------
#app.py

import os
import json
import datetime

import streamlit as st
import ollama

try:
    OLLAMA_MODELS = ollama.list()["models"]
except Exception as e:
    st.warning("Please make sure Ollama is installed first. See https://ollama.ai for more details.")
    st.stop()

def st_ollama(model_name, user_question, chat_history_key, params):
    if chat_history_key not in st.session_state.keys():
        st.session_state[chat_history_key] = []

    print_chat_history_timeline(chat_history_key)
        
    if user_question:
        st.session_state[chat_history_key].append({"content": f"{user_question}", "role": "user"})
        with st.chat_message("question", avatar="ğŸ§‘â€ğŸš€"):
            st.write(user_question)

        # íŒŒë¼ë¯¸í„° ì¶œë ¥
        with st.chat_message("parameters", avatar="ğŸ”§"):
            st.write("Ollama Parameters:")
            for key, value in params.items():
                if key != "system":  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” ë³„ë„ë¡œ í‘œì‹œ
                    st.write(f"{key}: {value}")
            st.write(f"System Prompt: {params.get('system', 'None')}")

        messages = [dict(content=message["content"], role=message["role"]) for message in st.session_state[chat_history_key]]
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ ì‹œì‘ ë¶€ë¶„ì— ì¶”ê°€
        if params.get("system"):
            messages.insert(0, {"role": "system", "content": params["system"]})

        def llm_stream(response):
            response = ollama.chat(
                model_name, 
                messages, 
                stream=True,
                options={k: v for k, v in params.items() if k != "system"}  # system í”„ë¡¬í”„íŠ¸ëŠ” optionsì—ì„œ ì œì™¸
            )
            for chunk in response:
                yield chunk['message']['content']

        with st.chat_message("response", avatar="ğŸ¤–"):
            chat_box = st.empty()
            response_message = chat_box.write_stream(llm_stream(messages))

        st.session_state[chat_history_key].append({"content": f"{response_message}", "role": "assistant"})
        
        return response_message

def print_chat_history_timeline(chat_history_key):
    for message in st.session_state[chat_history_key]:
        role = message["role"]
        if role == "user":
            with st.chat_message("user", avatar="ğŸ§‘â€ğŸš€"): 
                question = message["content"]
                st.markdown(f"{question}", unsafe_allow_html=True)
        elif role == "assistant":
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                st.markdown(message["content"], unsafe_allow_html=True)

def assert_models_installed():
    if len(OLLAMA_MODELS) < 1:
        st.sidebar.warning("No models found. Please install at least one model e.g. `ollama run llama2`")
        st.stop()

def select_model(key):
    model_names = ["ì„ íƒì•ˆí•¨"] + [model["name"] for model in OLLAMA_MODELS]
    default_index = model_names.index('gemma2:9b') if 'gemma2:9b' in model_names else 0
    llm_name = st.sidebar.selectbox(f"Choose Agent for {key}", model_names, index=default_index, key=f"model_select_{key}")
    if llm_name and llm_name != "ì„ íƒì•ˆí•¨":
        llm_details = [model for model in OLLAMA_MODELS if model["name"] == llm_name][0]
        if type(llm_details["size"]) != str:
            llm_details["size"] = f"{round(llm_details['size'] / 1e9, 2)} GB"
        with st.expander(f"LLM Details for {key}"):
            st.write(llm_details)
    return llm_name

def save_conversation(llm_name, conversation_key):
    OUTPUT_DIR = "llm_conversations"
    OUTPUT_DIR = os.path.join(os.getcwd(), OUTPUT_DIR)
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    filename = f"{OUTPUT_DIR}/{timestamp}_{llm_name.replace(':', '-')}"

    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    if st.session_state[conversation_key]:
        if st.sidebar.button(f"Save conversation {conversation_key}"):
            with open(f"{filename}_{conversation_key}.json", "w") as f:
                json.dump(st.session_state[conversation_key], f, indent=4)
            st.success(f"Conversation saved to {filename}_{conversation_key}.json")

if __name__ == "__main__":
    st.set_page_config(layout="wide", page_title="Ollama Chat", page_icon="ğŸ¦™")

    st.sidebar.title("Ollama Chat ğŸ¦™")
    
    assert_models_installed()

    # Model 1 ì„ íƒ
    llm_name_1 = select_model("Model 1")
    
    # Model 2 ì„ íƒ
    llm_name_2 = select_model("Model 2")

    # Model 1 Parameters
    # Model 1 Parameters
    params_1 = {}
    if llm_name_1 != "ì„ íƒì•ˆí•¨":
        st.sidebar.subheader("Model 1 Parameters")
        system_prompt_1 = st.sidebar.text_area("System Prompt for Model 1", "ì‚¬ìš©ìì˜ ë¬¸ì¥ì„ ì™„ì„±í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ê²°ê³¼ë§Œ ì œê³µí•´ì£¼ì„¸ìš”.", help="ëª¨ë¸ì˜ ì „ë°˜ì ì¸ í–‰ë™ì„ ì§€ì‹œ. êµ¬ì²´ì ì¼ìˆ˜ë¡ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê¸° ì‰¬ì›€", height=100)
        params_1 = {
            "system": system_prompt_1,
            "num_predict": st.sidebar.number_input("Max Tokens (num_predict) 1", min_value=1, max_value=2048, value=1024, step=1, help="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜. ë†’ì„ìˆ˜ë¡ ê¸´ ì‘ë‹µ, ë‚®ì„ìˆ˜ë¡ ì§§ì€ ì‘ë‹µ"),
            "temperature": st.sidebar.slider("Temperature 1", 0.0, 2.0, 0.0, 0.1, help="ë†’ì„ìˆ˜ë¡ ì°½ì˜ì„±ê³¼ ë‹¤ì–‘ì„± ì¦ê°€, ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„±ê³¼ ì •í™•ì„± ì¦ê°€"),
            "top_k": st.sidebar.slider("Top K 1", 1, 100, 40, 1, help="ë‹¤ìŒ í† í° ì„ íƒ ì‹œ ê³ ë ¤í•  ìƒìœ„ Kê°œì˜ í† í°. ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€, ë‚®ì„ìˆ˜ë¡ ì •í™•ì„± ì¦ê°€"),
            "top_p": st.sidebar.slider("Top P 1", 0.0, 1.0, 0.9, 0.05, help="ëˆ„ì  í™•ë¥  Pì— í•´ë‹¹í•˜ëŠ” ìƒìœ„ í† í°ë§Œ ê³ ë ¤. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •í™•ì„± ì¦ê°€"),
            "repeat_penalty": st.sidebar.slider("Repeat Penalty 1", 0.0, 2.0, 1.1, 0.1, help="ë‹¨ì–´ ë°˜ë³µ ì–µì œ. ë†’ì„ìˆ˜ë¡ ë°˜ë³µ ê°ì†Œ, ë‚®ì„ìˆ˜ë¡ ìì—°ìŠ¤ëŸ¬ìš´ ë°˜ë³µ í—ˆìš©"),
            "presence_penalty": st.sidebar.slider("Presence Penalty 1", 0.0, 2.0, 0.0, 0.1, help="ìƒˆë¡œìš´ ì£¼ì œ ë„ì… ì¥ë ¤. ë†’ì„ìˆ˜ë¡ ìƒˆë¡œìš´ ì£¼ì œ ë„ì… ì¦ê°€, ë‚®ì„ìˆ˜ë¡ ê¸°ì¡´ ì£¼ì œ ìœ ì§€"),
            "frequency_penalty": st.sidebar.slider("Frequency Penalty 1", 0.0, 2.0, 0.0, 0.1, help="ìì£¼ ì‚¬ìš©ëœ ë‹¨ì–´ ì–µì œ. ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ì–´íœ˜ ì‚¬ìš©, ë‚®ì„ìˆ˜ë¡ ìì—°ìŠ¤ëŸ¬ìš´ ë‹¨ì–´ ë¹ˆë„ ìœ ì§€"),
            "stop": st.sidebar.text_input("Stop Sequences 1 (comma-separated)", "", help="ìƒì„± ì¤‘ë‹¨ ì‹œí€€ìŠ¤. ì—¬ëŸ¬ ê°œ ì…ë ¥ ì‹œ ì‰¼í‘œë¡œ êµ¬ë¶„"),
            "seed": 1
        }
        params_1["stop"] = [s.strip() for s in params_1["stop"].split(',') if s.strip()]

    # Model 2 Parameters
    params_2 = {}
    if llm_name_2 != "ì„ íƒì•ˆí•¨":
        st.sidebar.subheader("Model 2 Parameters")
        system_prompt_2 = st.sidebar.text_area("System Prompt for Model 2", "ì‚¬ìš©ìì˜ ë¬¸ì¥ì„ ì™„ì„±í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ê²°ê³¼ë§Œ ì œê³µí•´ì£¼ì„¸ìš”.", help="ëª¨ë¸ì˜ ì „ë°˜ì ì¸ í–‰ë™ì„ ì§€ì‹œ. êµ¬ì²´ì ì¼ìˆ˜ë¡ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê¸° ì‰¬ì›€", height=100)
        params_2 = {
            "system": system_prompt_2,
            "num_predict": st.sidebar.number_input("Max Tokens (num_predict) 2", min_value=1, max_value=2048, value=1024, step=1, help="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜. ë†’ì„ìˆ˜ë¡ ê¸´ ì‘ë‹µ, ë‚®ì„ìˆ˜ë¡ ì§§ì€ ì‘ë‹µ"),
            "temperature": st.sidebar.slider("Temperature 2", 0.0, 2.0, 2.0, 0.1, help="ë†’ì„ìˆ˜ë¡ ì°½ì˜ì„±ê³¼ ë‹¤ì–‘ì„± ì¦ê°€, ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„±ê³¼ ì •í™•ì„± ì¦ê°€"),
            "top_k": st.sidebar.slider("Top K 2", 1, 100, 40, 1, help="ë‹¤ìŒ í† í° ì„ íƒ ì‹œ ê³ ë ¤í•  ìƒìœ„ Kê°œì˜ í† í°. ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€, ë‚®ì„ìˆ˜ë¡ ì •í™•ì„± ì¦ê°€"),
            "top_p": st.sidebar.slider("Top P 2", 0.0, 1.0, 0.9, 0.05, help="ëˆ„ì  í™•ë¥  Pì— í•´ë‹¹í•˜ëŠ” ìƒìœ„ í† í°ë§Œ ê³ ë ¤. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •í™•ì„± ì¦ê°€"),
            "repeat_penalty": st.sidebar.slider("Repeat Penalty 2", 0.0, 2.0, 1.1, 0.1, help="ë‹¨ì–´ ë°˜ë³µ ì–µì œ. ë†’ì„ìˆ˜ë¡ ë°˜ë³µ ê°ì†Œ, ë‚®ì„ìˆ˜ë¡ ìì—°ìŠ¤ëŸ¬ìš´ ë°˜ë³µ í—ˆìš©"),
            "presence_penalty": st.sidebar.slider("Presence Penalty 2", 0.0, 2.0, 0.0, 0.1, help="ìƒˆë¡œìš´ ì£¼ì œ ë„ì… ì¥ë ¤. ë†’ì„ìˆ˜ë¡ ìƒˆë¡œìš´ ì£¼ì œ ë„ì… ì¦ê°€, ë‚®ì„ìˆ˜ë¡ ê¸°ì¡´ ì£¼ì œ ìœ ì§€"),
            "frequency_penalty": st.sidebar.slider("Frequency Penalty 2", 0.0, 2.0, 0.0, 0.1, help="ìì£¼ ì‚¬ìš©ëœ ë‹¨ì–´ ì–µì œ. ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ì–´íœ˜ ì‚¬ìš©, ë‚®ì„ìˆ˜ë¡ ìì—°ìŠ¤ëŸ¬ìš´ ë‹¨ì–´ ë¹ˆë„ ìœ ì§€"),
            "stop": st.sidebar.text_input("Stop Sequences 2 (comma-separated)", "", help="ìƒì„± ì¤‘ë‹¨ ì‹œí€€ìŠ¤. ì—¬ëŸ¬ ê°œ ì…ë ¥ ì‹œ ì‰¼í‘œë¡œ êµ¬ë¶„"),
            "seed": 41
        }
        params_2["stop"] = [s.strip() for s in params_2["stop"].split(',') if s.strip()]

    prompt = st.chat_input("Ask a question ...")

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("Model 1 Output")
        if llm_name_1 != "ì„ íƒì•ˆí•¨":
            conversation_key_1 = f"model_{llm_name_1}_1"
            st_ollama(llm_name_1, prompt, conversation_key_1, params_1)
            
            # Clear and Save buttons for Model 1
            if st.session_state.get(conversation_key_1):
                clear_conversation_1 = st.sidebar.button("Clear chat 1")
                if clear_conversation_1:
                    st.session_state[conversation_key_1] = []
                    st.rerun()
            save_conversation(llm_name_1, conversation_key_1)
        else:
            st.write("ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    with col2:
        st.subheader("Model 2 Output")
        if llm_name_2 != "ì„ íƒì•ˆí•¨":
            conversation_key_2 = f"model_{llm_name_2}_2"
            st_ollama(llm_name_2, prompt, conversation_key_2, params_2)
            
            # Clear and Save buttons for Model 2
            if st.session_state.get(conversation_key_2):
                clear_conversation_2 = st.sidebar.button("Clear chat 2")
                if clear_conversation_2:
                    st.session_state[conversation_key_2] = []
                    st.rerun()
            save_conversation(llm_name_2, conversation_key_2)
        else:
            st.write("ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
------------------------------------------------------------------------------------------------------------------------
#ë¬¸ì„œì •ì œlaw.ipynb
!pip install pdfminer.six

from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfpage import PDFPage
from io import StringIO
import re

def convert_pdf_to_txt():
    rsrcmgr = PDFResourceManager()
    retstr = StringIO()
    codec = 'utf-8'
    laparams = LAParams()
    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)
    fp = open('law1.pdf', 'rb')
    interpreter = PDFPageInterpreter(rsrcmgr, device)
    password = ""
    maxpages = 999
    caching = True
    pagenos=set()

    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):
        interpreter.process_page(page)

    text = retstr.getvalue()

    fp.close()
    device.close()
    retstr.close()
    return text

v = convert_pdf_to_txt()
print(v)

def preprocess_document(document):

    # 3. \n\nì„ [para]ë¡œ ë³€ê²½
    # document = re.sub(r'ë²•ì œì²˜\s*\d+\s*êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°\s*ë¯¼ë²•', '', document)
    # document = re.sub(r'ë²•ì œì²˜\s*\d+\s*êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°\s', '', document)
    pattern = re.compile(r'\x0c')  # '\x0c'ëŠ” FORM FEED ë¬¸ìë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    document = pattern.sub('', document)
    
    document = document.replace('           ì œ', '[ê´€]ì œ')
    document = document.replace('         ì œ', '[ì ˆ]ì œ')
    document = document.replace('       ì œ', '[ì¥]ì œ')
    document = document.replace('     ì œ', '[í¸]ì œ')        
    document = document.replace('\n\nì œ', '[ì¡°]ì œ')
    document = document.replace('\nì œ', '[ì¡°]ì œ')
    document = document.replace('\n\n', ' ')
    document = document.replace('\n ', ' ')
          
    document = re.sub(r'ë²•ì œì²˜\s*.*', '', document)
    
    # ë¬¸ì„œë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    lines = document.split('\n')
    # ê° ì¤„ì˜ ê°€ì¥ ì•ì— ìˆëŠ” 'ë¯¼ë²•'ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ì‚­ì œ
    for i in range(len(lines)):
        lines[i] = re.sub(r'^\s*ë¯¼ë²•\s*', '', lines[i])   
    # ì¬ê²°í•©
    document = '\n'.join(lines)
    
    document = document.replace(' \n', ' ')
    document = document.replace('[ì¡°]', '\n')    
    document = document.replace('\nì œ325ì¡°ì˜', ' ì œ325ì¡°ì˜')
    document = document.replace('[í¸]', '\n\n')
    document = document.replace('[ì¥]', '\n\n\n')
    document = document.replace('[ì ˆ]', '\n\n\n\n')
    document = document.replace('[ê´€]', '\n\n\n\n\n')
    
    # ë¼ì¸ë³„ ë¶„ë¦¬
    lines = document.split('\n')
    # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ 'ì œXì¡°'ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ì„ '[ì¡°]ì œXì¡°'ë¡œ ë³€ê²½
    for i in range(len(lines)):
        lines[i] = re.sub(r'^(ì œ\d+ì¡°)', r'[ì¡°]\1', lines[i])
        lines[i] = re.sub(r'^(ì œ\d+í¸)', r'[í¸]\1', lines[i])
        lines[i] = re.sub(r'^(ì œ\d+ì¥)', r'[ì¥]\1', lines[i])
        lines[i] = re.sub(r'^(ì œ\d+ì ˆ)', r'[ì ˆ]\1', lines[i])
        lines[i] = re.sub(r'^(ì œ\d+ê´€)', r'[ê´€]\1', lines[i])
    # ë‹¤ì‹œ ê²°í•©
    document = '\n'.join(lines)    
    
    document = document.replace('\nì œ', ' ì œ')    
    document = document.replace('[ì¡°]', '')    
    document = document.replace('[í¸]', '')
    document = document.replace('[ì¥]', '')
    document = document.replace('[ì ˆ]', '')
    document = document.replace('[ê´€]', '')
    return document    

text = preprocess_document(v)
print(text)

#ë¬¸ì„œë¡œ ì €ì¥
with open('law1.txt', 'w', encoding='utf-8') as file:
    file.write(text)

print("law1.txtì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
------------------------------------------------------------------------------------------------------------------------
#ë¬¸ì„œì •ì œ2.ipynb
pip install olefile

import olefile
import zlib
import struct
 
def get_hwp_text(filename):
    f = olefile.OleFileIO(filename)
    dirs = f.listdir()
 
    if ["FileHeader"] not in dirs or \
            ["\x05HwpSummaryInformation"] not in dirs:
        raise Exception("Not Valid HWP.")
 
    header = f.openstream("FileHeader")
    header_data = header.read()
    is_compressed = (header_data[36] & 1) == 1
 
    nums = []
    for d in dirs:
        if d[0] == "BodyText":
            nums.append(int(d[1][len("Section"):]))
    sections = ["BodyText/Section" + str(x) for x in sorted(nums)]
 
    text = ""
    for section in sections:
        bodytext = f.openstream(section)
        data = bodytext.read()
        if is_compressed:
            unpacked_data = zlib.decompress(data, -15)
        else:
            unpacked_data = data
 
        section_text = ""
        i = 0
        size = len(unpacked_data)
        while i < size:
            header = struct.unpack_from("<I", unpacked_data, i)[0]
            rec_type = header & 0x3ff
            rec_len = (header >> 20) & 0xfff
 
            if rec_type in [67]:
                rec_data = unpacked_data[i + 4:i + 4 + rec_len]
                section_text += rec_data.decode('utf-16')
                section_text += "\n"
            i += 4 + rec_len
 
        text += section_text
        text += "\n"
    
    return text
    
text = get_hwp_text('sample2.hwp')

#ë¬¸ì„œ ì •ì œ
import re

def clean_text(text):
    # ê°€-í£, a-zA-Z, 0-9, ê´„í˜¸ (), <>, ë”°ì˜´í‘œ "", êµ¬ë‘ì (. , : [ ] ! ? -)ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì ì œê±°
    cleaned_text = re.sub(r'[^ê°€-í£a-zA-Z0-9()<>\"\".,:\s\[\]]', '', text)
    # 'ë¶€ì¹™' ì´í›„ì˜ ëª¨ë“  ë‚´ìš© ì œê±°
    cleaned_text = re.sub(r'ë¶€\s*ì¹™.*', '', cleaned_text, flags=re.DOTALL)
    return cleaned_text

cleaned_text = clean_text(text)  # í…ìŠ¤íŠ¸ ì •ì œ
print(cleaned_text)

#ë¬¸ì„œë¡œ ì €ì¥
with open('law2.txt', 'w', encoding='utf-8') as file:
    file.write(cleaned_text)

print("law2.txtì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
